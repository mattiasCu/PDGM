{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from torchdrug import  layers, datasets,transforms,core, models\n",
    "from torchdrug.core import Registry as R\n",
    "from torchdrug.layers import geometry\n",
    "\n",
    "import torch\n",
    "from torchdrug import data\n",
    "\n",
    "from torch_scatter import scatter_add\n",
    "import torch.nn as nn\n",
    "from torchdrug import utils\n",
    "from torch.utils import checkpoint\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from einops import rearrange, repeat, pack, unpack\n",
    "from collections.abc import Sequence\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchdrug.layers import functional\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "LARGE_NUMBER = 1.e10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:29:40   Extracting /home/cu/scratch/protein-datasets/EnzymeCommission.zip to /home/cu/scratch/protein-datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading /home/cu/scratch/protein-datasets/EnzymeCommission/enzyme_commission.pkl.gz: 100%|██████████| 18716/18716 [00:45<00:00, 408.21it/s]\n"
     ]
    }
   ],
   "source": [
    "EnzymeCommission = R.search(\"datasets.EnzymeCommission\")\n",
    "PV = R.search(\"transforms.ProteinView\")\n",
    "trans = PV(view = \"residue\")\n",
    "dataset = EnzymeCommission(\"~/scratch/protein-datasets/\", test_cutoff=0.95, \n",
    "                           atom_feature=\"full\", bond_feature=\"full\", verbose=1, transform = trans)\n",
    "\n",
    "# 只保留alpha碳的简化格式\n",
    "graph_construction_model = layers.GraphConstruction(node_layers=[geometry.AlphaCarbonNode()], \n",
    "                                                    edge_layers=[geometry.SpatialEdge(radius=10.0, min_distance=5),\n",
    "                                                                 geometry.KNNEdge(k=10, min_distance=5),\n",
    "                                                                 geometry.SequentialEdge(max_distance=2)\n",
    "                                                                 ],\n",
    "                                                    edge_feature=\"gearnet\"\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedProtein(batch_size=2, num_atoms=[185, 415], num_bonds=[3754, 8999], num_residues=[185, 415])\n"
     ]
    }
   ],
   "source": [
    "graphs = dataset[0:2]\n",
    "graphs = [element[\"graph\"] for element in graphs]\n",
    "graphs = data.Protein.pack(graphs)\n",
    "graph = graph_construction_model(graphs)\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([185, 415])\n",
      "tensor([[ 95,  96,   5],\n",
      "        [109, 110,   5],\n",
      "        [108, 109,   5],\n",
      "        ...,\n",
      "        [438, 470,   0],\n",
      "        [489, 470,   0],\n",
      "        [493, 470,   0]])\n",
      "torch.Size([12753, 3])\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(graph.num_nodes)\n",
    "edge_list = graph.edge_list \n",
    "\n",
    "print(edge_list)\n",
    "print(edge_list.shape)\n",
    "\n",
    "print(graph.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relationnal conv graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relationalGraphConv(layers.MessagePassingBase):\n",
    "    \n",
    "    eps = 1e-10\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, num_relation, edge_input_dim=None, batch_norm=False, activation=\"relu\"):\n",
    "        super(relationalGraphConv, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_relation = num_relation\n",
    "        self.edge_input_dim = edge_input_dim\n",
    "\n",
    "        if batch_norm:\n",
    "            self.batch_norm = nn.BatchNorm1d(output_dim)\n",
    "        else:\n",
    "            self.batch_norm = None\n",
    "        if isinstance(activation, str):\n",
    "            self.activation = getattr(F, activation)\n",
    "        else:\n",
    "            self.activation = activation\n",
    "\n",
    "        self.self_loop = nn.Linear(input_dim, output_dim)\n",
    "        self.linear = nn.Linear(num_relation * input_dim, output_dim)\n",
    "        if edge_input_dim:\n",
    "            self.edge_linear = nn.Linear(edge_input_dim, input_dim)\n",
    "        else:\n",
    "            self.edge_linear = None\n",
    "\n",
    "\n",
    "    def message_and_aggregate(self, graph, input):\n",
    "        assert graph.num_relation == self.num_relation\n",
    "        device = input.device  # Ensure device consistency\n",
    "        \n",
    "        node_in, node_out, relation = graph.edge_list.t().to(device)\n",
    "        node_out = node_out * self.num_relation + relation\n",
    "    \n",
    "        degree_out = scatter_add(graph.edge_weight, node_out, dim_size=graph.num_node * graph.num_relation)\n",
    "        edge_weight = graph.edge_weight / degree_out[node_out]\n",
    "        adjacency = utils.sparse_coo_tensor(torch.stack([node_in, node_out]), edge_weight,\n",
    "                                            (graph.num_node, graph.num_node * graph.num_relation))\n",
    "        update = torch.sparse.mm(adjacency.t().to(device), input.to(device))\n",
    "        \n",
    "        if self.edge_linear:\n",
    "            edge_input = graph.edge_feature.float().to(device)\n",
    "            edge_input = self.edge_linear(edge_input)\n",
    "            edge_weight = edge_weight.unsqueeze(-1).to(device)\n",
    "            edge_update = scatter_add(edge_input * edge_weight, node_out, dim=0,\n",
    "                                      dim_size=graph.num_node * graph.num_relation)\n",
    "            update += edge_update\n",
    "\n",
    "        return update.view(input.size(0), self.num_relation * self.input_dim)                           \n",
    "\n",
    "\n",
    "    def combine(self, input, update):\n",
    "        device = input.device\n",
    "        self.linear.to(device)  # Ensure the linear layers are on the correct device\n",
    "        self.self_loop.to(device)\n",
    "        output = self.linear(update) + self.self_loop(input)\n",
    "        if self.batch_norm:\n",
    "            self.batch_norm.to(device)\n",
    "            output = self.batch_norm(output)\n",
    "        if self.activation:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, graph, input):\n",
    "        device = input.device\n",
    "        if self.gradient_checkpoint:\n",
    "            update = checkpoint.checkpoint(self._message_and_aggregate, *graph.to_tensors(), input)\n",
    "        else:\n",
    "            update = self.message_and_aggregate(graph.to(device), input)\n",
    "        output = self.combine(input, update)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relationalGraph(layers.MessagePassingBase):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, num_relation, edge_input_dim=None, batch_norm=False, activation=\"relu\"):\n",
    "        super(relationalGraph, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_relation = num_relation\n",
    "        self.edge_input_dim = edge_input_dim\n",
    "\n",
    "        if batch_norm:\n",
    "            self.batch_norm = nn.BatchNorm1d(output_dim)\n",
    "        else:\n",
    "            self.batch_norm = None\n",
    "        if isinstance(activation, str):\n",
    "            self.activation = getattr(F, activation)\n",
    "        else:\n",
    "            self.activation = activation\n",
    "\n",
    "        self.self_loop = nn.Linear(input_dim, output_dim)\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        if edge_input_dim:\n",
    "            self.edge_linear = nn.Linear(edge_input_dim, input_dim)\n",
    "        else:\n",
    "            self.edge_linear = None\n",
    "        \n",
    "    def message_and_aggregate(self, graph, input):\n",
    "        assert graph.num_relation == self.num_relation\n",
    "        device = input.device  # Ensure device consistency\n",
    "        \n",
    "\n",
    "        node_in, node_out, relation = graph.edge_list.t().to(device)\n",
    "        node_out = node_out * self.num_relation + relation\n",
    "    \n",
    "        degree_out = scatter_add(graph.edge_weight, node_out, dim_size=graph.num_node * graph.num_relation)\n",
    "        edge_weight = graph.edge_weight / degree_out[node_out]\n",
    "        adjacency = utils.sparse_coo_tensor(torch.stack([node_in, node_out]), edge_weight,\n",
    "                                            (graph.num_node, graph.num_node * graph.num_relation))\n",
    "        update = torch.sparse.mm(adjacency.t().to(device), input.to(device))\n",
    "        \n",
    "        if self.edge_linear:\n",
    "            edge_input = graph.edge_feature.float().to(device)\n",
    "            edge_input = self.edge_linear(edge_input)\n",
    "            edge_weight = edge_weight.unsqueeze(-1).to(device)\n",
    "            edge_update = scatter_add(edge_input * edge_weight, node_out, dim=0,\n",
    "                                      dim_size=graph.num_node * graph.num_relation)\n",
    "            update += edge_update\n",
    "\n",
    "        return update.view(input.size(0), self.num_relation, self.input_dim).permute(1, 0, 2).reshape(self.num_relation*input.size(0), self.input_dim)\n",
    "\n",
    "    def combine(self, input, update):\n",
    "        # 自环特征\n",
    "        device = input.device\n",
    "        self.linear.to(device)  # Ensure the linear layers are on the correct device\n",
    "        self.self_loop.to(device)\n",
    "        input = input.repeat(self.num_relation, 1).to(device)\n",
    "        loop_update = self.self_loop(input).to(device)\n",
    "        \n",
    "        output = self.linear(update)+loop_update\n",
    "        if self.batch_norm:\n",
    "            self.batch_norm.to(device)\n",
    "            output = self.batch_norm(output)\n",
    "        if self.activation:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, graph, input):\n",
    "        device = input.device\n",
    "        if self.gradient_checkpoint:\n",
    "            update = checkpoint.checkpoint(self._message_and_aggregate, *graph.to_tensors(), input)\n",
    "        else:\n",
    "            update = self.message_and_aggregate(graph.to(device), input)\n",
    "        output = self.combine(input, update)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relationalGraphStack(nn.Module):\n",
    "    \n",
    "    def __init__(self, dims, num_relation, edge_input_dim=None, batch_norm=True, activation=\"relu\"):\n",
    "        super(relationalGraphStack, self).__init__()\n",
    "        self.num_layers = len(dims) - 1\n",
    "        self.layers = nn.ModuleList()\n",
    "        if self.num_layers > 1:\n",
    "            for i in range(self.num_layers-1):\n",
    "                self.layers.append(relationalGraphConv(dims[i], dims[i + 1], num_relation, edge_input_dim, batch_norm, activation))\n",
    "            \n",
    "        self.layers.append(relationalGraph(dims[-2], dims[-1], num_relation, edge_input_dim, batch_norm, activation))\n",
    "            \n",
    "\n",
    "    def forward(self, graph, input, new_edge_list=None):\n",
    "        device = input.device\n",
    "        x = input\n",
    "        for layer in self.layers:\n",
    "            x = layer(graph.to(device), x)         \n",
    "        return x.reshape(graph.num_relation, input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 600, 512])\n"
     ]
    }
   ],
   "source": [
    "rel_dims = [[21, 512, 512]]\n",
    "num_relations = graph.num_relation\n",
    "\n",
    "model = relationalGraphStack(rel_dims[0], num_relations,batch_norm=True)\n",
    "relational_output = model(graph, graph.node_feature.float().to(device))\n",
    "print(relational_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 图重连部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rewirescorelayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_relations,num_heads, window_size, k, temperature=0.5):\n",
    "        super(Rewirescorelayer, self).__init__()\n",
    "        self.input_dim = in_features\n",
    "        self.output_dim = out_features\n",
    "        self.num_relations = num_relations\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.k = k\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        self.query = nn.Linear(in_features, out_features * num_heads)\n",
    "        self.key = nn.Linear(in_features, out_features * num_heads)\n",
    "        self.scale = 1 / (out_features ** 0.5)\n",
    "    \n",
    "    \n",
    "    \n",
    "    class LocalAttention(nn.Module):\n",
    "        def __init__(\n",
    "            self,\n",
    "            window_size,\n",
    "            look_backward = 1,\n",
    "            look_forward = None,\n",
    "            dropout = 0.,\n",
    "            dim = None,\n",
    "            scale = None,\n",
    "            pad_start_position = None\n",
    "        ):\n",
    "            super().__init__()\n",
    "\n",
    "            self.scale = scale\n",
    "\n",
    "            self.window_size = window_size\n",
    "\n",
    "            self.look_backward = look_backward\n",
    "            self.look_forward = look_forward\n",
    "            \n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.pad_start_position = pad_start_position\n",
    "            \n",
    "            \n",
    "\n",
    "        def exists(self,val):\n",
    "            return val is not None\n",
    "\n",
    "        # 如果value不存在，返回d\n",
    "        def default(self,value, d):\n",
    "            return d if not self.exists(value) else value\n",
    "\n",
    "        def to(self, t):\n",
    "            return {'device': t.device, 'dtype': t.dtype}\n",
    "\n",
    "        def max_neg_value(self, tensor):\n",
    "            return -torch.finfo(tensor.dtype).max  #返回给定张量数据类型的所能表示的最大负值\n",
    "\n",
    "        def look_around(self, x, backward = 1, forward = 0, pad_value = -1, dim = 2):  #x = bk: (40, 32, 16, 64)\n",
    "            t = x.shape[1]    #获取一共有多少个窗口，这里是32\n",
    "            dims = (len(x.shape) - dim) * (0, 0)   #一个长度为 len(x.shape) - dim 的元组，每个元素为 (0, 0)；其中len(x.shape) = 4\n",
    "            padded_x = F.pad(x, (*dims, backward, forward), value = pad_value)   #在第二维度上，前面加backward个元素，后面加forward个元素 -> (40, 33, 16, 64)\n",
    "            tensors = [padded_x[:, ind:(ind + t), ...] for ind in range(forward + backward + 1)] #一个张量列表，每个张量的维度为(40, 32, 16, 64), len = 2\n",
    "            return torch.cat(tensors, dim = dim) #在第二维度上拼接 -> (40, 32, 32, 64)\n",
    "    \n",
    "        def forward(\n",
    "            self,\n",
    "            q, k,\n",
    "            mask = None,\n",
    "            input_mask = None,\n",
    "            window_size = None\n",
    "        ):\n",
    "\n",
    "            mask = self.default(mask, input_mask)\n",
    "            assert not (self.exists(window_size) and not self.use_xpos), 'cannot perform window size extrapolation if xpos is not turned on'\n",
    "            shape, pad_value, window_size, look_backward, look_forward = q.shape, -1, self.default(window_size, self.window_size), self.look_backward, self.look_forward\n",
    "            (q, packed_shape), (k, _) = map(lambda t: pack([t], '* n d'), (q, k))  #打包成[5, 8, 512, 64] -> [40, 512, 64] \n",
    "\n",
    "\n",
    "            b, n, dim_head, device, dtype = *q.shape, q.device, q.dtype   # 40, 512, 64\n",
    "            scale = self.default(self.scale, dim_head ** -0.5)\n",
    "            assert (n % window_size) == 0, f'sequence length {n} must be divisible by window size {window_size} for local attention'\n",
    "\n",
    "            windows = n // window_size  # 512 / 16 = 32\n",
    "\n",
    "            seq = torch.arange(n, device = device)                  # 0, 1, 2, 3, ..., 511\n",
    "            b_t = rearrange(seq, '(w n) -> 1 w n', w = windows, n = window_size)    # (1, 32, 16) 排序序列变形后的矩阵\n",
    "\n",
    "            # bucketing\n",
    "\n",
    "            bq, bk = map(lambda t: rearrange(t, 'b (w n) d -> b w n d', w = windows), (q, k)) #重构：（40，512，64）->（40, 32, 16, 64）\n",
    "\n",
    "            bq = bq * scale    # (40, 32, 16, 64)\n",
    " \n",
    "            look_around_kwargs = dict(\n",
    "                backward =  look_backward,\n",
    "                forward =  look_forward,\n",
    "                pad_value = pad_value\n",
    "            )\n",
    "\n",
    "            bk = self.look_around(bk, **look_around_kwargs)      # (40, 32, 32, 64)\n",
    "    \n",
    "\n",
    "            # calculate positions for masking\n",
    "\n",
    "            bq_t = b_t\n",
    "            bq_k = self.look_around(b_t, **look_around_kwargs) # (1, 32, 32)\n",
    "\n",
    "            bq_t = rearrange(bq_t, '... i -> ... i 1')      # (1, 32, 16, 1)\n",
    "            bq_k = rearrange(bq_k, '... j -> ... 1 j')      # (1, 32, 1, 16)\n",
    "\n",
    "            pad_mask = bq_k == pad_value\n",
    "\n",
    "            sim = torch.einsum('b h i e, b h j e -> b h i j', bq, bk)  # (40, 32, 16, 64) * (40, 32, 32, 64) -> (40, 32, 16, 32)\n",
    "\n",
    "            mask_value = self.max_neg_value(sim)\n",
    "\n",
    "            sim = sim.masked_fill(pad_mask, mask_value)\n",
    "\n",
    "\n",
    "            if self.exists(mask):\n",
    "                batch = mask.shape[0]    # 5\n",
    "                assert (b % batch) == 0\n",
    "\n",
    "                h = b // mask.shape[0]  # 8\n",
    "\n",
    "                mask = rearrange(mask, '... (w n) -> (...) w n', w = windows, n = window_size)\n",
    "                mask = self.look_around(mask, **{**look_around_kwargs, 'pad_value': False})\n",
    "                mask = rearrange(mask, '... j -> ... 1 j')\n",
    "                mask = repeat(mask, 'b ... -> (b h) ...', h = h)\n",
    "\n",
    "                sim = sim.masked_fill(~mask, mask_value)\n",
    "                del mask\n",
    "                \n",
    "            indices = [self.pad_start_position[i] // window_size for i in range(len(self.pad_start_position)) if i % 2 != 0]\n",
    "            all_indices = list(range(windows))\n",
    "            remaining_indices = [idx for idx in all_indices if idx not in indices]\n",
    "            \n",
    "            # 使用剩余的索引选择元素\n",
    "            rest_sim = sim[:, remaining_indices, :, :]\n",
    "\n",
    "            # attention\n",
    "            attn = rest_sim.softmax(dim = -1)\n",
    "            attn = self.dropout(attn)\n",
    "            \n",
    "            return attn\n",
    "\n",
    "    def insert_zero_rows(self, tensor, lengths, target_lengths):\n",
    "        assert len(lengths) == len(target_lengths), \"Lengths and target lengths must be of the same length.\"\n",
    "        \n",
    "        # 计算每个位置需要插入的零行数\n",
    "        zero_rows = [target - length for length, target in zip(lengths, target_lengths)]\n",
    "        \n",
    "        # 初始化结果列表\n",
    "        parts = []\n",
    "        mask_parts = []\n",
    "        start = 0\n",
    "        \n",
    "        for i, length in enumerate(lengths):\n",
    "            end = start + length\n",
    "            \n",
    "            # 原始张量部分\n",
    "            parts.append(tensor[:, start:end, :])\n",
    "            mask_parts.append(torch.ones(tensor.size(0), length, dtype=torch.bool, device=tensor.device))\n",
    "            \n",
    "            # 插入零行\n",
    "            if zero_rows[i] > 0:\n",
    "                zero_padding = torch.zeros(tensor.size(0), zero_rows[i], tensor.size(2), device=tensor.device)\n",
    "                mask_padding = torch.zeros(tensor.size(0), zero_rows[i], dtype=torch.bool, device=tensor.device)\n",
    "                parts.append(zero_padding)\n",
    "                mask_parts.append(mask_padding)\n",
    "            \n",
    "            start = end\n",
    "        \n",
    "        # 拼接所有部分\n",
    "        padded_tensor = torch.cat(parts, dim=1)\n",
    "        mask = torch.cat(mask_parts, dim=1)\n",
    "        \n",
    "        return padded_tensor, mask\n",
    "\n",
    "\n",
    "    def round_up_to_nearest_k_and_a_window_size(self, lst, k):\n",
    "        pad_start_position = []\n",
    "        result_lst = [(x + k - 1) // k * k +k for x in lst]\n",
    "        for i in range(len(lst)):\n",
    "            pad_start_position.append(sum(result_lst[:i])-i*k + lst[i])\n",
    "            pad_start_position.append(sum(result_lst[:i+1])-k)\n",
    "        return result_lst, pad_start_position\n",
    "\n",
    "        \n",
    "    def displace_tensor_blocks_to_rectangle(self, tensor, displacement):\n",
    "        batch_size, num_blocks, block_height, block_width = tensor.shape\n",
    "\n",
    "        # 计算新矩阵的宽度和高度\n",
    "        height = num_blocks * displacement\n",
    "        width =  (2 + num_blocks) * displacement\n",
    "\n",
    "        # 初始化新的大张量，确保其形状为 (batch_size, height, width)\n",
    "        new_tensor = torch.zeros(batch_size, height, width, device=tensor.device, dtype=tensor.dtype)\n",
    "\n",
    "        for i in range(num_blocks):\n",
    "            start_pos_height = i * displacement\n",
    "            start_pos_width = i * displacement\n",
    "            end_pos_height = start_pos_height + block_height\n",
    "            end_pos_width = start_pos_width + block_width\n",
    "\n",
    "            new_tensor[:, start_pos_height:end_pos_height, start_pos_width:end_pos_width] = tensor[:, i, :, :]\n",
    "\n",
    "        return new_tensor\n",
    "    \n",
    "    def forward(self, graph, node_features):\n",
    "        \n",
    "        device = node_features.device\n",
    "        num_relation = self.num_relations\n",
    "        index = graph.num_nodes.tolist()\n",
    "        \n",
    "        target_input, pad_start_position = self.round_up_to_nearest_k_and_a_window_size(index, self.window_size)\n",
    "        padding_input, mask = self.insert_zero_rows(node_features, index, target_input)\n",
    "        \n",
    "        self.query = self.query.to(device)\n",
    "        self.key = self.key.to(device)\n",
    "        Q = self.query(padding_input).view(num_relation, padding_input.size(1), self.num_heads, self.output_dim).permute(0, 2, 1, 3)                           # (num_relations, num_nodes, num_heads, out_features\n",
    "        K = self.key(padding_input).view(num_relation, padding_input.size(1), self.num_heads, self.output_dim).permute(0, 2, 1, 3)                             # (num_relations, num_nodes, num_heads, out_features)\n",
    "        Q = Q.reshape(num_relation * self.num_heads, padding_input.size(1), self.output_dim)                                                  # (num_relations*num_heads, num_nodes, out_features)\n",
    "        K = K.reshape(num_relation * self.num_heads, padding_input.size(1), self.output_dim) \n",
    "        \n",
    "        attn = self.LocalAttention(\n",
    "            dim = self.output_dim,                   # dimension of each head (you need to pass this in for relative positional encoding)\n",
    "            window_size = self.window_size,          # window size. 512 is optimal, but 256 or 128 yields good enough results\n",
    "            look_backward = 1,                  # each window looks at the window before\n",
    "            look_forward = 1,                   # for non-auto-regressive case, will default to 1, so each window looks at the window before and after it\n",
    "            dropout = 0.1,\n",
    "            pad_start_position = pad_start_position\n",
    "            \n",
    "        ) \n",
    "        \n",
    "        attn = attn(Q, K, mask = mask).view(num_relation, self.num_heads, -1, self.window_size, 3*self.window_size).mean(dim=1) \n",
    "        \n",
    "        #======================================================================================================\n",
    "        \"\"\"\n",
    "        attn = attn.reshape(-1, self.window_size, 3*self.window_size)\n",
    "\n",
    "        # 确保 -LARGE_NUMBER 的类型与 result_tensor 相同\n",
    "        large_negative_number = torch.tensor(-LARGE_NUMBER, dtype=attn.dtype, device=attn.device)\n",
    "        score = torch.where(attn==0, large_negative_number, attn)\n",
    "        model = GumbleSampler(self.k, tau=1, hard=True)\n",
    "        score = model(score)\n",
    "        val = model.validate(score)\n",
    "        \n",
    "        accuracy = (score == val).float().mean().item()\n",
    "        #print(f'Accuracy: {accuracy:.4f}')\n",
    "        \n",
    "        score = score.reshape(num_relation, -1, self.window_size, 3*self.window_size)\n",
    "        \"\"\"\n",
    "        #=======================================================================================================\n",
    "        \n",
    "        score = attn\n",
    "        result_tensor = self.displace_tensor_blocks_to_rectangle(score, self.window_size)\n",
    "        result_tensor = result_tensor[:, :, self.window_size:-self.window_size]\n",
    "        indice = [pad_start_position[i] for i in range(len(pad_start_position)) if i % 2 == 0]\n",
    "        indices = []\n",
    "\n",
    "        for num in indice:\n",
    "            next_multiple_of_window_size = ((num + self.window_size-1) // self.window_size) * self.window_size  # 计算向上取10的倍数\n",
    "            sequence = range(num, next_multiple_of_window_size)  # 生成序列\n",
    "            indices.extend(sequence)  # 直接将序列中的元素添加到结果列表中\n",
    "        all_indices = list(range(result_tensor.size(1)))\n",
    "        remaining_indices = [idx for idx in all_indices if idx not in indices]\n",
    "        \n",
    "        result_tensor = result_tensor[:, remaining_indices, :]\n",
    "        result_tensor = result_tensor[:, :, remaining_indices]\n",
    "\n",
    "        \n",
    "        return result_tensor.permute(1, 0, 2).contiguous().view(result_tensor.size(1), result_tensor.size(0)*result_tensor.size(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "运行时间: 0.004401 秒\n",
      "torch.Size([600, 4200])\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "input_dim = relational_output.shape[-1]\n",
    "output_dim = 64\n",
    "num_realtions = 7\n",
    "num_heads = 8\n",
    "window_size = 10\n",
    "k = 3\n",
    "num_nodes = relational_output.size(0)\n",
    "\n",
    "model = Rewirescorelayer(input_dim, output_dim, num_relations, num_heads, window_size, k)\n",
    "start = time.time()\n",
    "attn_output = model(graph, relational_output)\n",
    "end = time.time()\n",
    "print(f\"运行时间: {end - start:.6f} 秒\")\n",
    "print(attn_output.shape)\n",
    "print(attn_output.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重构图结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphRewiring(nn.Module, core.Configurable):\n",
    "\n",
    "    max_seq_dist = 10\n",
    "\n",
    "    def __init__(self,  edge_feature=\"gearnet\"):\n",
    "        super(GraphRewiring, self).__init__()\n",
    "        \n",
    "        self.edge_feature = edge_feature\n",
    "        \n",
    "    \n",
    "    def adjacency_matrix_to_edge_list(self, A):\n",
    "        # 找到非零元素的索引\n",
    "        idx = torch.nonzero(A, as_tuple=False) \n",
    "        i = idx[:, 0]  \n",
    "        j = idx[:, 1]\n",
    "        line_num = A.size(0)  \n",
    "        \n",
    "        # 计算关系编号（relation）和调整后的列索引（j_prime）\n",
    "        relation = torch.div(j, line_num, rounding_mode='floor')  # 关系编号，0到6\n",
    "        j_prime = j % line_num  # 调整后的列索引，0到184\n",
    "        \n",
    "        # 组合得到edge_list矩阵，形状为[n, 3]\n",
    "        edge_list = torch.stack([i, j_prime, relation], dim=1)\n",
    "        \n",
    "        # 根据edge_list的索引顺序，获取对应的edge_weighted矩阵\n",
    "        edge_weighted = A[i, j]\n",
    "        \n",
    "        return edge_list, edge_weighted\n",
    "\n",
    "    \n",
    "\n",
    "    def edge_gearnet(self, graph, edge_list, num_relation):\n",
    "        node_in, node_out, r = edge_list.t()\n",
    "        residue_in, residue_out = graph.atom2residue[node_in], graph.atom2residue[node_out]\n",
    "        in_residue_type = graph.residue_type[residue_in]\n",
    "        out_residue_type = graph.residue_type[residue_out]\n",
    "        sequential_dist = torch.abs(residue_in - residue_out)\n",
    "        spatial_dist = (graph.node_position[node_in] - graph.node_position[node_out]).norm(dim=-1)\n",
    "\n",
    "        return torch.cat([\n",
    "            functional.one_hot(in_residue_type, len(data.Protein.residue2id)),\n",
    "            functional.one_hot(out_residue_type, len(data.Protein.residue2id)),\n",
    "            functional.one_hot(r, num_relation),\n",
    "            functional.one_hot(sequential_dist.clamp(max=self.max_seq_dist), self.max_seq_dist + 1),\n",
    "            spatial_dist.unsqueeze(-1)\n",
    "        ], dim=-1)\n",
    "\n",
    "    def apply_node_layer(self, graph):\n",
    "        \n",
    "        return graph\n",
    "\n",
    "    def apply_edge_layer(self, graph, edge_list, edge_weighted):\n",
    "        \n",
    "        node_in = edge_list[:, 0]\n",
    "        edge2graph = graph.node2graph[node_in]\n",
    "        order = edge2graph.argsort()\n",
    "        edge_list = edge_list[order]\n",
    "        num_edges = edge2graph.bincount(minlength=graph.batch_size)\n",
    "        offsets = (graph.num_cum_nodes - graph.num_nodes).repeat_interleave(num_edges)\n",
    "        num_relation = graph.num_relation\n",
    "\n",
    "        if hasattr(self, \"edge_%s\" % self.edge_feature):\n",
    "            edge_feature = getattr(self, \"edge_%s\" % self.edge_feature)(graph, edge_list, num_relation)\n",
    "        elif self.edge_feature is None:\n",
    "            edge_feature = None\n",
    "        else:\n",
    "            raise ValueError(\"Unknown edge feature `%s`\" % self.edge_feature)\n",
    "        data_dict, meta_dict = graph.data_by_meta(include=(\n",
    "            \"node\", \"residue\", \"node reference\", \"residue reference\", \"graph\"\n",
    "        ))\n",
    "\n",
    "        if isinstance(graph, data.PackedProtein):\n",
    "            data_dict[\"num_residues\"] = graph.num_residues\n",
    "        if isinstance(graph, data.PackedMolecule):\n",
    "            data_dict[\"bond_type\"] = torch.zeros_like(edge_list[:, 2])\n",
    "        return type(graph)(edge_list, num_nodes=graph.num_nodes, num_edges=num_edges, num_relation=num_relation,\n",
    "                           view=graph.view, offsets=offsets, edge_feature=edge_feature, edge_weight=edge_weighted,\n",
    "                           meta_dict=meta_dict, **data_dict)\n",
    "\n",
    "    def forward(self, graph, attn_output):\n",
    "        \"\"\"\n",
    "        Generate a new graph based on the input graph and pre-defined node and edge layers.\n",
    "\n",
    "        Parameters:\n",
    "            graph (Graph): :math:`n` graph(s)\n",
    "\n",
    "        Returns:\n",
    "            graph (Graph): new graph(s)\n",
    "        \"\"\"\n",
    "        device = graph.device\n",
    "        graph = self.apply_node_layer(graph)\n",
    "        edge_list, edge_weighted = self.adjacency_matrix_to_edge_list(attn_output)\n",
    "        graph = self.apply_edge_layer(graph, edge_list.to(device), edge_weighted.to(device))\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([122850, 59])\n"
     ]
    }
   ],
   "source": [
    "rewiring = GraphRewiring(edge_feature=\"gearnet\")\n",
    "new_graph = rewiring(graph, attn_output)\n",
    "print(new_graph.edge_feature.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DGMGearnet模型架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGMGearnet_edge(nn.Module, core.Configurable):\n",
    "\n",
    "    def __init__(self, relation_dims, score_in_dim, score_out_dim, diffusion_dims, num_relation, attn_num_relation, num_heads, window_size, k, edge_input_dim=None, num_angle_bin=None,\n",
    "                 short_cut=False, batch_norm=False, activation=\"relu\", concat_hidden=False, edge_feature = \"gearnet\", readout=\"sum\"):\n",
    "        super(DGMGearnet_edge, self).__init__()\n",
    "\n",
    "        self.relation_dims = relation_dims\n",
    "        self.score_in_dim = score_in_dim\n",
    "        self.score_out_dim = score_out_dim  \n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.k = k\n",
    "        self.num_relation = num_relation\n",
    "        self.attn_num_relation = attn_num_relation\n",
    "        self.space_information_num = self.num_relation - attn_num_relation\n",
    "\n",
    "        self.dims = diffusion_dims\n",
    "        self.diffusion_input_dim = sum([[sublist[0] for sublist in diffusion_dims]], [])\n",
    "        self.diffusion_dims = [sublist[1:] for sublist in diffusion_dims]\n",
    "        self.edge_input_dim = edge_input_dim\n",
    "        self.num_angle_bin = num_angle_bin\n",
    "        self.edge_feature = edge_feature\n",
    "        \n",
    "        self.short_cut = short_cut\n",
    "        self.concat_hidden = concat_hidden\n",
    "        self.batch_norm = batch_norm\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        self.score_layers = nn.ModuleList()\n",
    "        self.graph_construction = GraphRewiring(self.edge_feature)\n",
    "        for i in range(len(self.relation_dims)):\n",
    "            if i == 0:\n",
    "                self.score_layers.append(relationalGraphStack(self.relation_dims[i], num_relation, \n",
    "                                                        edge_input_dim=None, batch_norm=True, activation=\"relu\")) \n",
    "\n",
    "            else:\n",
    "                self.score_layers.append(relationalGraphStack(self.relation_dims[i], num_relation, \n",
    "                                                        edge_input_dim=None, batch_norm=True, activation=\"relu\")) \n",
    "                    \n",
    "            self.score_layers.append(Rewirescorelayer(self.score_in_dim, self.score_out_dim, attn_num_relation,self.num_heads, self.window_size, \n",
    "                                                        self.k, temperature=0.5))\n",
    "            \n",
    "            self.layers.append(models.GearNet(self.diffusion_input_dim[i], self.diffusion_dims[i], self.num_relation, self.edge_input_dim, self.num_angle_bin,\n",
    "                                                batch_norm=self.batch_norm, concat_hidden=False, short_cut=False, readout=\"sum\"))\n",
    "        \n",
    "\n",
    "        if batch_norm:\n",
    "            self.batch_norms = nn.ModuleList()\n",
    "            for i in range(len(self.dims) ):\n",
    "                self.batch_norms.append(nn.BatchNorm1d(self.dims[i][-1]))\n",
    "\n",
    "        if readout == \"sum\":\n",
    "            self.readout = layers.SumReadout()\n",
    "        elif readout == \"mean\":\n",
    "            self.readout = layers.MeanReadout()\n",
    "        else:\n",
    "            raise ValueError(\"Unknown readout `%s`\" % readout)\n",
    "\n",
    "    def forward(self, graph, input, edge_list=None, all_loss=None, metric=None):\n",
    "        device = input.device\n",
    "        node_in, node_out, relation = graph.edge_list.t().to(device)\n",
    "        node_out = node_out * self.num_relation + relation\n",
    "        adjacency = torch.sparse_coo_tensor(\n",
    "            torch.stack([node_in, node_out]),\n",
    "            graph.edge_weight.to(device),\n",
    "            (graph.num_node, graph.num_node * graph.num_relation),\n",
    "            device=device\n",
    "        )\n",
    "        adjacency = adjacency.to_dense()\n",
    "        adjacency = adjacency.t().view(graph.num_node, graph.num_relation, graph.num_node).permute(1, 0, 2).reshape(graph.num_relation*graph.num_node , graph.num_node).t()\n",
    "        \n",
    "        hiddens = []\n",
    "        layer_input = input\n",
    "        score_layer_input = input\n",
    "        all_loss = 0\n",
    "        \n",
    "        \"\"\"\n",
    "        if self.num_angle_bin:\n",
    "            line_graph = self.spatial_line_graph(graph)\n",
    "            edge_input = line_graph.node_feature.float()\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(len(self.layers)):\n",
    "            \n",
    "            \n",
    "            relational_output = self.score_layers[2*i](graph, score_layer_input)\n",
    "            \n",
    "            attn_input = relational_output[self.space_information_num:, :, :]    \n",
    "            attn_output = self.score_layers[2*i+1](graph, attn_input)\n",
    "            \n",
    "            if self.space_information_num != 0:\n",
    "                attn_output = torch.max(adjacency[:, self.space_information_num *adjacency.size(1)//self.num_relation:], attn_output)\n",
    "                attn_output = torch.cat([attn_output, adjacency[:, :self.space_information_num *adjacency.size(1)//self.num_relation]], dim=1)\n",
    "            \n",
    "            new_graph = self.graph_construction(graph, attn_output)\n",
    "            output = self.layers[i](new_graph.to(device), layer_input.to(device))\n",
    "            hidden = output[\"node_feature\"]\n",
    "            \n",
    "            if self.short_cut and hidden.shape == layer_input.shape:\n",
    "                hidden = hidden + layer_input\n",
    "\n",
    "                \n",
    "            if self.batch_norm:\n",
    "                hidden = self.batch_norms[i](hidden)\n",
    "            \n",
    "            hiddens.append(hidden)\n",
    "            score_layer_input = torch.cat([hidden, relational_output.view(hidden.size(0), self.num_relation*hidden.size(-1))], dim=-1)\n",
    "            layer_input = hidden\n",
    "\n",
    "            graph = new_graph\n",
    "\n",
    "        if self.concat_hidden:\n",
    "            node_feature = torch.cat(hiddens, dim=-1)\n",
    "        else:\n",
    "            node_feature = hiddens[-1]\n",
    "        graph_feature = self.readout(graph, node_feature)\n",
    "\n",
    "        return {\n",
    "            \"graph_feature\": graph_feature,\n",
    "            \"node_feature\": node_feature,\n",
    "        }   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[117], line 16\u001b[0m\n\u001b[1;32m     11\u001b[0m num_angle_bin \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m DGMGearnet_edge(relation_dims, score_in_dim, score_out_dim, diffusion_dims, num_relations, attn_num_relation, num_heads, window_size, k, \n\u001b[1;32m     14\u001b[0m                         edge_input_dim, num_angle_bin, batch_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m, concat_hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, edge_feature\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgearnet\u001b[39m\u001b[38;5;124m\"\u001b[39m, readout\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_feature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/graph/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[116], line 96\u001b[0m, in \u001b[0;36mDGMGearnet_edge.forward\u001b[0;34m(self, graph, input, edge_list, all_loss, metric)\u001b[0m\n\u001b[1;32m     93\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([attn_output, adjacency[:, :\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspace_information_num \u001b[38;5;241m*\u001b[39madjacency\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_relation]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     95\u001b[0m new_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_construction(graph, attn_output)\n\u001b[0;32m---> 96\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m hidden \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode_feature\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshort_cut \u001b[38;5;129;01mand\u001b[39;00m hidden\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m layer_input\u001b[38;5;241m.\u001b[39mshape:\n",
      "File \u001b[0;32m~/anaconda3/envs/graph/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/graph/lib/python3.10/site-packages/torchdrug/models/gearnet.py:95\u001b[0m, in \u001b[0;36mGeometryAwareRelationalGraphNeuralNetwork.forward\u001b[0;34m(self, graph, input, all_loss, metric)\u001b[0m\n\u001b[1;32m     92\u001b[0m     edge_input \u001b[38;5;241m=\u001b[39m line_graph\u001b[38;5;241m.\u001b[39mnode_feature\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers)):\n\u001b[0;32m---> 95\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshort_cut \u001b[38;5;129;01mand\u001b[39;00m hidden\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m layer_input\u001b[38;5;241m.\u001b[39mshape:\n\u001b[1;32m     97\u001b[0m         hidden \u001b[38;5;241m=\u001b[39m hidden \u001b[38;5;241m+\u001b[39m layer_input\n",
      "File \u001b[0;32m~/anaconda3/envs/graph/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/graph/lib/python3.10/site-packages/torchdrug/layers/conv.py:92\u001b[0m, in \u001b[0;36mMessagePassingBase.forward\u001b[0;34m(self, graph, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     update \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessage_and_aggregate(graph, \u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m---> 92\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/anaconda3/envs/graph/lib/python3.10/site-packages/torchdrug/layers/conv.py:436\u001b[0m, in \u001b[0;36mRelationalGraphConv.combine\u001b[0;34m(self, input, update)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcombine\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, update):\n\u001b[0;32m--> 436\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_loop(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_norm:\n\u001b[1;32m    438\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_norm(output)\n",
      "File \u001b[0;32m~/anaconda3/envs/graph/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/graph/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "relation_dims = [[21, 128, 512, 512], [4096, 1024, 512, 512]]\n",
    "score_in_dim = 512\n",
    "score_out_dim = 512\n",
    "diffusion_dims = [[21, 512, 512, 512], [512, 512, 512, 512]]  \n",
    "num_relations = graph.num_relation\n",
    "attn_num_relation = 2\n",
    "num_heads = 8\n",
    "window_size = 10\n",
    "k = 5\n",
    "edge_input_dim = 59\n",
    "num_angle_bin = 8\n",
    "\n",
    "model = DGMGearnet_edge(relation_dims, score_in_dim, score_out_dim, diffusion_dims, num_relations, attn_num_relation, num_heads, window_size, k, \n",
    "                        edge_input_dim, num_angle_bin, batch_norm=True, activation=\"relu\", concat_hidden=True, edge_feature=\"gearnet\", readout=\"sum\")\n",
    "\n",
    "output = model(graph, graph.node_feature.float().to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 画图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 90,  82,   1],\n",
      "        [ 85,  78,   1],\n",
      "        [ 83,  78,   1],\n",
      "        ...,\n",
      "        [ 75, 107,   0],\n",
      "        [ 50, 108,   0],\n",
      "        [ 51, 108,   0]])\n",
      "[(59, 60), (45, 46), (46, 47), (47, 48), (48, 49), (49, 50), (50, 51), (51, 52), (52, 53), (53, 54), (54, 55), (55, 56), (56, 57), (57, 58), (58, 59), (44, 45), (60, 61), (61, 62), (62, 63), (63, 64), (64, 65), (65, 66), (66, 67), (67, 68), (68, 69), (69, 70), (70, 71), (71, 72), (72, 73), (73, 74), (30, 31), (16, 17), (17, 18), (18, 19), (19, 20), (20, 21), (21, 22), (22, 23), (23, 24), (24, 25), (25, 26), (26, 27), (27, 28), (28, 29), (29, 30), (74, 75), (31, 32), (32, 33), (33, 34), (34, 35), (35, 36), (36, 37), (37, 38), (38, 39), (39, 40), (40, 41), (41, 42), (42, 43), (43, 44), (118, 119), (104, 105), (105, 106), (106, 107), (107, 108), (108, 109), (109, 110), (110, 111), (111, 112), (112, 113), (113, 114), (114, 115), (115, 116), (116, 117), (117, 118), (103, 104), (119, 120), (120, 121), (121, 122), (122, 123), (123, 124), (124, 125), (125, 126), (126, 127), (127, 128), (128, 129), (129, 130), (130, 131), (131, 132), (132, 133), (89, 90), (75, 76), (76, 77), (77, 78), (78, 79), (79, 80), (80, 81), (81, 82), (82, 83), (83, 84), (84, 85), (85, 86), (86, 87), (87, 88), (88, 89), (15, 16), (90, 91), (91, 92), (92, 93), (93, 94), (94, 95), (95, 96), (96, 97), (97, 98), (98, 99), (99, 100), (100, 101), (101, 102), (102, 103), (1, 2), (0, 1), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 15), (133, 134), (177, 178), (163, 164), (164, 165), (165, 166), (166, 167), (167, 168), (168, 169), (169, 170), (170, 171), (171, 172), (172, 173), (173, 174), (174, 175), (175, 176), (176, 177), (162, 163), (178, 179), (179, 180), (180, 181), (181, 182), (182, 183), (183, 184), (148, 149), (134, 135), (135, 136), (136, 137), (137, 138), (138, 139), (139, 140), (140, 141), (141, 142), (142, 143), (143, 144), (144, 145), (145, 146), (146, 147), (147, 148), (149, 150), (150, 151), (151, 152), (152, 153), (153, 154), (154, 155), (155, 156), (156, 157), (157, 158), (158, 159), (159, 160), (160, 161), (161, 162)]\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "num_nodes = graph.num_nodes\n",
    "edge_list = graph.edge_list\n",
    "print(edge_list)\n",
    "# 1. 过滤行，只保留每行最后一个数为1的行\n",
    "filtered_rows = edge_list[edge_list[:, 2] == 5]\n",
    "\n",
    "# 2. 去掉最后一列，只保留前两列\n",
    "filtered_rows = filtered_rows[:, :2]\n",
    "\n",
    "# 3. 将结果转换为一对对的形式\n",
    "edge_list = [tuple(pair) for pair in filtered_rows.tolist()]\n",
    "print(edge_list)\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(edge_list)\n",
    "plt.figure(figsize=(10, 10))  # 设置画布大小\n",
    "nx.draw(G, with_labels=True, node_size=70, node_color='lightblue', font_size=6, font_color='black', edge_color='black')\n",
    "plt.title('Graph Visualization')\n",
    "plt.savefig('tu.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
